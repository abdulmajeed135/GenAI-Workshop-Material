{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph for Lemmatization\n",
    "\n",
    "paragraph = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lemmatization object\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank much .\n",
      "Thank Academy .\n",
      "Thank room .\n",
      "I congratulate incredible nominee year .\n",
      "The Revenant product tireless effort unbelievable cast crew .\n",
      "First , brother endeavor , Mr. Tom Hardy .\n",
      "Tom , talent screen surpassed friendship screen … thank creating ranscendent cinematic experience .\n",
      "Thank everybody Fox New Regency entire team .\n",
      "I thank everyone onset career … To parent ; none would possible without .\n",
      "And friend , I love dearly ; know .\n",
      "And lastly , I want say : Making The Revenant man 's relationship natural world .\n",
      "A world collectively felt 2015 hottest year recorded history .\n",
      "Our production needed move southern tip planet able find snow .\n",
      "Climate change real , happening right .\n",
      "It urgent threat facing entire specie , need work collectively together stop procrastinating .\n",
      "We need support leader around world speak big polluter , speak humanity , indigenous people world , billion billion underprivileged people would affected .\n",
      "For child ’ child , people whose voice drowned politics greed .\n",
      "I thank amazing award tonight .\n",
      "Let u take planet granted .\n",
      "I take tonight granted .\n",
      "Thank much .\n"
     ]
    }
   ],
   "source": [
    "# Applying lemmatization\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)      \n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
