{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acomF9mvLHZK"
      },
      "source": [
        "# Document Question Answering with local persistence\n",
        "\n",
        "An example of using Chroma DB and LangChain to do question answering over documents, with a locally persisted database.\n",
        "You can store embeddings and documents, then use them again later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5nOuzobLSIi",
        "outputId": "750f9a34-7010-47e9-812f-aac862e52aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (23.0.1)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "     ---------------------------------------- 2.1/2.1 MB 8.4 MB/s eta 0:00:00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: To modify pip, please run the following command:\n",
            "C:\\Users\\Abdul\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.0.2.post1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-openai) (1.24.3)\n",
            "Collecting langchain-core<0.2,>=0.1.7\n",
            "  Downloading langchain_core-0.1.11-py3-none-any.whl (218 kB)\n",
            "     -------------------------------------- 218.6/218.6 kB 4.4 MB/s eta 0:00:00\n",
            "Collecting tiktoken<0.6.0,>=0.5.2\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-win_amd64.whl (786 kB)\n",
            "     -------------------------------------- 786.3/786.3 kB 7.1 MB/s eta 0:00:00\n",
            "Collecting openai<2.0.0,>=1.6.1\n",
            "  Downloading openai-1.8.0-py3-none-any.whl (222 kB)\n",
            "     -------------------------------------- 222.3/222.3 kB 6.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (6.0.1)\n",
            "Collecting packaging<24.0,>=23.2\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "     ---------------------------------------- 53.0/53.0 kB ? eta 0:00:00\n",
            "Requirement already satisfied: anyio<5,>=3 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (2.28.1)\n",
            "Collecting langsmith<0.1.0,>=0.0.63\n",
            "  Downloading langsmith-0.0.81-py3-none-any.whl (48 kB)\n",
            "     ---------------------------------------- 48.4/48.4 kB ? eta 0:00:00\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (8.2.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.2,>=0.1.7->langchain-openai) (1.33)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (4.66.1)\n",
            "Collecting distro<2,>=1.7.0\n",
            "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (4.9.0)\n",
            "Collecting httpx<1,>=0.23.0\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "     ---------------------------------------- 75.9/75.9 kB 4.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: sniffio in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<2.0.0,>=1.6.1->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken<0.6.0,>=0.5.2->langchain-openai) (2023.10.3)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain-openai) (1.1.3)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain-openai) (3.4)\n",
            "Collecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "     ---------------------------------------- 76.9/76.9 kB 2.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai) (2022.9.24)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.7->langchain-openai) (2.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.7->langchain-openai) (1.26.12)\n",
            "Requirement already satisfied: colorama in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.6.1->langchain-openai) (0.4.5)\n",
            "Installing collected packages: packaging, httpcore, distro, tiktoken, langsmith, httpx, openai, langchain-core, langchain-openai\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 20.9\n",
            "    Uninstalling packaging-20.9:\n",
            "      Successfully uninstalled packaging-20.9\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.0.44\n",
            "    Uninstalling langsmith-0.0.44:\n",
            "      Successfully uninstalled langsmith-0.0.44\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.1\n",
            "    Uninstalling openai-0.28.1:\n",
            "      Successfully uninstalled openai-0.28.1\n",
            "Successfully installed distro-1.9.0 httpcore-1.0.2 httpx-0.26.0 langchain-core-0.1.11 langchain-openai-0.0.2.post1 langsmith-0.0.81 openai-1.8.0 packaging-23.2 tiktoken-0.5.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tfx 1.14.0 requires kubernetes<13,>=10.0.1, but you have kubernetes 29.0.0 which is incompatible.\n",
            "tfx 1.14.0 requires packaging<21,>=20, but you have packaging 23.2 which is incompatible.\n",
            "tensorflow-intel 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "ml-pipelines-sdk 1.14.0 requires packaging<21,>=20, but you have packaging 23.2 which is incompatible.\n",
            "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chromadb==0.3.29\n",
            "  Using cached chromadb-0.3.29-py3-none-any.whl (396 kB)\n",
            "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from chromadb==0.3.29) (7.4.0)\n",
            "Requirement already satisfied: clickhouse-connect>=0.5.7 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (0.6.6)\n",
            "Collecting hnswlib>=0.7\n",
            "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from chromadb==0.3.29) (1.16.3)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from chromadb==0.3.29) (3.4.0)\n",
            "Requirement already satisfied: requests>=2.28 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (2.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (1.24.3)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (0.23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (4.9.0)\n",
            "Collecting fastapi==0.85.1\n",
            "  Using cached fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (4.66.1)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (1.10.13)\n",
            "Requirement already satisfied: duckdb>=0.7.1 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (0.7.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from chromadb==0.3.29) (3.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (0.14.1)\n",
            "Requirement already satisfied: pandas>=1.3 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb==0.3.29) (1.5.3)\n",
            "Collecting starlette==0.20.4\n",
            "  Using cached starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (6.11.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2022.9.24)\n",
            "Requirement already satisfied: zstandard in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (0.22.0)\n",
            "Requirement already satisfied: pytz in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2022.2.1)\n",
            "Requirement already satisfied: lz4 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (4.3.3)\n",
            "Requirement already satisfied: urllib3>=1.26 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (1.26.12)\n",
            "Requirement already satisfied: packaging in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.2)\n",
            "Requirement already satisfied: protobuf in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (3.20.3)\n",
            "Requirement already satisfied: sympy in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (1.12)\n",
            "Requirement already satisfied: coloredlogs in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.5.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3->chromadb==0.3.29) (2.8.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.16.0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from posthog>=2.4.0->chromadb==0.3.29) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.29) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.28->chromadb==0.3.29) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tokenizers>=0.13.2->chromadb==0.3.29) (0.17.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.65.0->chromadb==0.3.29) (0.4.5)\n",
            "Requirement already satisfied: click>=7.0 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.14.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (6.0.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (1.0.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.21.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.6.1)\n",
            "Requirement already satisfied: websockets>=10.4 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (12.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (3.12.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (2023.9.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.29) (10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from importlib-metadata->clickhouse-connect>=0.5.7->chromadb==0.3.29) (3.17.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.29) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.1.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\abdul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.3.0)\n",
            "Requirement already satisfied: pyreadline3 in c:\\users\\abdul\\appdata\\roaming\\python\\python310\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.29) (3.4.1)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml): started\n",
            "  Building wheel for hnswlib (pyproject.toml): finished with status 'error'\n",
            "Failed to build hnswlib\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for hnswlib (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [5 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_ext\n",
            "      building 'hnswlib' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for hnswlib\n",
            "ERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q langchain\n",
        "!pip install -U langchain-openai\n",
        "!pip install chromadb==0.3.29\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3vPToAU5Nh5r"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "import chromadb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ELekNXAmLHZL"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "#from langchain-openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain.llms import OpenAI\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import VectorDBQA\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrXy8tV3LHZM"
      },
      "source": [
        "## Load and process documents\n",
        "\n",
        "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace.\n",
        "\n",
        "Next we split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LJ2IIpmpLHZM"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error loading state_of_the_union.txt",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\document_loaders\\text.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1205: character maps to <undefined>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19236\\1182036675.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load and process the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state_of_the_union.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext_splitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Abdul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\document_loaders\\text.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error loading {self.file_path}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error loading {self.file_path}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Error loading state_of_the_union.txt"
          ]
        }
      ],
      "source": [
        "# Load and process the text\n",
        "loader = TextLoader('state_of_the_union.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-TrcO4wLHZM"
      },
      "source": [
        "## Initialize PeristedChromaDB\n",
        "\n",
        "Create embeddings for each chunk and insert into the Chroma vector database. The `persist_directory` argument tells ChromaDB where to store the database when it's persisted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zdFzhPmLHZM"
      },
      "outputs": [],
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "OPENAI_API_KEY = \"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\" # enter your OpenAI key\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "molL_wHGLHZM"
      },
      "source": [
        "## Persist the Database\n",
        "In a notebook, we should call `persist()` to ensure the embeddings are written to disk.\n",
        "This isn't necessary in a script - the database will be automatically persisted when the client object is destroyed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt48DQ3dLHZM"
      },
      "outputs": [],
      "source": [
        "vectordb.persist()\n",
        "vectordb = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJYTtQR_LHZN"
      },
      "source": [
        "## Load the Database from disk, and create the chain\n",
        "Be sure to pass the same `persist_directory` and `embedding_function` as you did when you instantiated the database. Initialize the chain we will use for question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4_nGPISLHZN",
        "outputId": "0f15f755-d930-4cc0-9621-c1c1e59ad54e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py:256: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Now we can load the persisted database from disk, and use it as normal.\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key=OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=vectordb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyw4VVWOLHZN"
      },
      "source": [
        "## Ask questions!\n",
        "\n",
        "Now we can use the chain to ask questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ECOah01qLHZN",
        "outputId": "113dba55-4c10-4304-ade3-0fcf62e2cee8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson for the United States Supreme Court and praised her as one of the nation's top legal minds and a consensus builder with broad support from both Democrats and Republicans.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6iCt_XcLHZN"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "When you're done with the database, you can delete it from disk. You can delete the specific collection you're working with (if you have several), or delete the entire database by nuking the persistence directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAw0liTCLHZN",
        "outputId": "0d58da1a-1df3-4921-f3ad-d643763146bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Persisting DB to disk, putting it in the save folder db\n"
          ]
        }
      ],
      "source": [
        "# To cleanup, you can delete the collection\n",
        "vectordb.delete_collection()\n",
        "vectordb.persist()\n",
        "\n",
        "# Or just nuke the persist directory\n",
        "!rm -rf db/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "chroma-langchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c909e91d0cd7642213937968dfc91c71973575965f56cdcabb1e0b29abe5f7fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
